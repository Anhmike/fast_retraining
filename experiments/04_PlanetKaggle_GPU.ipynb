{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Experiment 04: Amazon Planet (GPU version)\n",
    "\n",
    "This experiment uses the data from the Kaggle competition [Planet: Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/leaderboard). Here we use a pretrained ResNet50 model to generate the features from the dataset.\n",
    "\n",
    "For details of virtual machine we used and the versions of LightGBM and XGBoost, please refer to [experiment 1](01_airline.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "XGBoost version: 0.6\n",
      "LightGBM version: 0.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from libs.loaders import load_planet_kaggle\n",
    "from libs.planet_kaggle import threshold_prediction\n",
    "from libs.timer import Timer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"XGBoost version: {}\".format(pkg_resources.get_distribution('xgboost').version))\n",
    "print(\"LightGBM version: {}\".format(pkg_resources.get_distribution('lightgbm').version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MOUNT_POINT=/datadrive\n"
     ]
    }
   ],
   "source": [
    "%env MOUNT_POINT=/datadrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Configure TF to use only one GPU, by default TF allocates memory in all GPUs\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1})\n",
    "#Configure TF to limit the amount of GPU memory, by default TF takes all of them. \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The images are loaded and featurised using a pretrained ResNet50 model available from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurising training images: 100%|██████████| 1094/1094.0 [07:09<00:00,  2.76it/s]\n",
      "Featurising validation images: 100%|██████████| 172/172.0 [01:07<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_planet_kaggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 2048)\n",
      "(35000, 17)\n",
      "(5479, 2048)\n",
      "(5479, 17)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will use a one-v-rest. So each classifier will be responsible for determining whether the assigned tag applies to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_validate_xgboost(params, train_features, train_labels, validation_features, num_boost_round):\n",
    "    n_classes = train_labels.shape[1]\n",
    "    y_val_pred = np.zeros((validation_features.shape[0], n_classes))\n",
    "    time_results = defaultdict(list)\n",
    "    for class_i in tqdm(range(n_classes)):\n",
    "        dtrain = xgb.DMatrix(data=train_features, label=train_labels[:, class_i])\n",
    "        dtest = xgb.DMatrix(data=validation_features)\n",
    "        with Timer() as t:\n",
    "            model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "        time_results['train_time'].append(t.interval)\n",
    "        \n",
    "        with Timer() as t:\n",
    "            y_val_pred[:, class_i] = model.predict(dtest)\n",
    "        time_results['test_time'].append(t.interval)\n",
    "        \n",
    "    return y_val_pred, time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_and_validate_lightgbm(params, train_features, train_labels, validation_features, num_boost_round):\n",
    "    n_classes = train_labels.shape[1]\n",
    "    y_val_pred = np.zeros((validation_features.shape[0], n_classes))\n",
    "    time_results = defaultdict(list)\n",
    "    for class_i in tqdm(range(n_classes)):\n",
    "        lgb_train = lgb.Dataset(train_features, train_labels[:, class_i], free_raw_data=False)\n",
    "        with Timer() as t:\n",
    "            model = lgb.train(params, lgb_train, num_boost_round = num_boost_round)\n",
    "        time_results['train_time'].append(t.interval)\n",
    "        \n",
    "        with Timer() as t:\n",
    "            y_val_pred[:, class_i] = model.predict(validation_features)\n",
    "        time_results['test_time'].append(t.interval)\n",
    "        \n",
    "    return y_val_pred, time_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': lambda y_true, y_pred: precision_score(y_true, y_pred, average='samples'),\n",
    "    'Recall': lambda y_true, y_pred: recall_score(y_true, y_pred, average='samples'),\n",
    "    'F1': lambda y_true, y_pred: f1_score(y_true, y_pred, average='samples'),\n",
    "}\n",
    "\n",
    "def classification_metrics(metrics, y_true, y_pred):\n",
    "    return {metric_name:metric(y_true, y_pred) for metric_name, metric in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_dict = dict()\n",
    "num_rounds = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we are going to define the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {'max_depth':3, \n",
    "              'objective':'binary:logistic', \n",
    "              'min_child_weight':1, \n",
    "              'eta':0.1, \n",
    "              'colsample_bytree':0.80,\n",
    "              'scale_pos_weight':2, \n",
    "              'gamma':0.1, \n",
    "              'reg_lamda':1, \n",
    "              'subsample':1,\n",
    "              'tree_method':'exact', \n",
    "              'updater':'grow_gpu'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [00:12<03:13, 12.06s/it]"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "b'[16:02:03] /home/hoaphumanoid/repos/xgboost/plugin/updater_gpu/src/updater_gpu.cc:40: GPU plugin exception: /home/hoaphumanoid/repos/xgboost/plugin/updater_gpu/src/device_helpers.cuh(363): out of memory\\n\\n\\nStack trace returned 10 entries:\\n[bt] (0) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN4dmlc15LogMessageFatalD1Ev+0x3c) [0x7fcc6538529c]\\n[bt] (1) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost4tree8GPUMakerINS0_9GradStatsEE6UpdateERKSt6vectorINS_9bst_gpairESaIS5_EEPNS_7DMatrixERKS4_IPNS_7RegTreeESaISD_EE+0x411) [0x7fcc65510af1]\\n[bt] (2) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost3gbm6GBTree13BoostNewTreesERKSt6vectorINS_9bst_gpairESaIS3_EEPNS_7DMatrixEiPS2_ISt10unique_ptrINS_7RegTreeESt14default_deleteISB_EESaISE_EE+0x8c3) [0x7fcc6541a5f3]\\n[bt] (3) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost3gbm6GBTree7DoBoostEPNS_7DMatrixEPSt6vectorINS_9bst_gpairESaIS5_EEPNS_11ObjFunctionE+0x86d) [0x7fcc6541b6bd]\\n[bt] (4) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE+0x22b) [0x7fcc6550276b]\\n[bt] (5) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(XGBoosterUpdateOneIter+0x27) [0x7fcc653772b7]\\n[bt] (6) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7fccff0cd370]\\n[bt] (7) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(ffi_call+0x1f5) [0x7fccff0ccb15]\\n[bt] (8) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(_ctypes_callproc+0x3dc) [0x7fccff0c45dc]\\n[bt] (9) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(+0x9c43) [0x7fccff0bcc43]\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4f8f3b7b7ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiming_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate_xgboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b8dceb0b6fea>\u001b[0m in \u001b[0;36mtrain_and_validate_xgboost\u001b[0;34m(params, train_features, train_labels, validation_features, num_boost_round)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtime_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \"\"\"\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[16:02:03] /home/hoaphumanoid/repos/xgboost/plugin/updater_gpu/src/updater_gpu.cc:40: GPU plugin exception: /home/hoaphumanoid/repos/xgboost/plugin/updater_gpu/src/device_helpers.cuh(363): out of memory\\n\\n\\nStack trace returned 10 entries:\\n[bt] (0) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN4dmlc15LogMessageFatalD1Ev+0x3c) [0x7fcc6538529c]\\n[bt] (1) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost4tree8GPUMakerINS0_9GradStatsEE6UpdateERKSt6vectorINS_9bst_gpairESaIS5_EEPNS_7DMatrixERKS4_IPNS_7RegTreeESaISD_EE+0x411) [0x7fcc65510af1]\\n[bt] (2) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost3gbm6GBTree13BoostNewTreesERKSt6vectorINS_9bst_gpairESaIS3_EEPNS_7DMatrixEiPS2_ISt10unique_ptrINS_7RegTreeESt14default_deleteISB_EESaISE_EE+0x8c3) [0x7fcc6541a5f3]\\n[bt] (3) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost3gbm6GBTree7DoBoostEPNS_7DMatrixEPSt6vectorINS_9bst_gpairESaIS5_EEPNS_11ObjFunctionE+0x86d) [0x7fcc6541b6bd]\\n[bt] (4) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(_ZN7xgboost11LearnerImpl13UpdateOneIterEiPNS_7DMatrixE+0x22b) [0x7fcc6550276b]\\n[bt] (5) /anaconda/envs/strata/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/libxgboost.so(XGBoosterUpdateOneIter+0x27) [0x7fcc653772b7]\\n[bt] (6) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(ffi_call_unix64+0x4c) [0x7fccff0cd370]\\n[bt] (7) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(ffi_call+0x1f5) [0x7fccff0ccb15]\\n[bt] (8) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(_ctypes_callproc+0x3dc) [0x7fccff0c45dc]\\n[bt] (9) /anaconda/envs/strata/lib/python3.5/lib-dynload/_ctypes.so(+0x9c43) [0x7fccff0bcc43]\\n'"
     ]
    }
   ],
   "source": [
    "y_pred, timing_results = train_and_validate_xgboost(xgb_params, X_train, y_train, X_test, num_boost_round=num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_dict['xgb']={\n",
    "    'train_time': np.sum(timing_results['train_time']),\n",
    "    'test_time': np.sum(timing_results['test_time']),\n",
    "    'performance': classification_metrics(metrics_dict, \n",
    "                                          y_test, \n",
    "                                          threshold_prediction(y_pred, threshold=0.1)) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb_hist_params = {'max_depth':0, \n",
    "                  'objective':'binary:logistic', \n",
    "                  'min_child_weight':1, \n",
    "                  'eta':0.1, \n",
    "                  'colsample_bytree':0.80,\n",
    "                  'scale_pos_weight':2, \n",
    "                  'gamma':0.1, \n",
    "                  'reg_lamda':1, \n",
    "                  'subsample':1,\n",
    "                  'tree_method':'hist', \n",
    "                  'max_leaves':2**6, \n",
    "                  'grow_policy':'lossguide',\n",
    "                  'max_bins': 63,\n",
    "                  'updater':'grow_gpu_hist'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 2/17 [04:53<36:48, 147.23s/it]"
     ]
    }
   ],
   "source": [
    "y_pred, timing_results = train_and_validate_xgboost(xgb_hist_params, X_train, y_train, X_test, num_boost_round=num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_dict['xgb_hist']={\n",
    "    'train_time': np.sum(timing_results['train_time']),\n",
    "    'test_time': np.sum(timing_results['test_time']),\n",
    "    'performance': classification_metrics(metrics_dict, \n",
    "                                          y_test, \n",
    "                                          threshold_prediction(y_pred, threshold=0.1)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {'num_leaves': 2**6,\n",
    "             'learning_rate': 0.1,\n",
    "             'scale_pos_weight': 2,\n",
    "             'min_split_gain': 0.1,\n",
    "             'min_child_weight': 1,\n",
    "             'reg_lambda': 1,\n",
    "             'subsample': 1,\n",
    "             'objective':'binary',\n",
    "             'max_bin': 63,\n",
    "             'device': 'gpu',\n",
    "             'task': 'train'\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred, timing_results = train_and_validate_lightgbm(lgb_params, X_train, y_train, X_test, num_boost_round=num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_dict['lgbm']={\n",
    "    'train_time': np.sum(timing_results['train_time']),\n",
    "    'test_time': np.sum(timing_results['test_time']),\n",
    "    'performance': classification_metrics(metrics_dict, \n",
    "                                          y_test, \n",
    "                                          threshold_prediction(y_pred, threshold=0.1)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "print(json.dumps(results_dict, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This dataset shows an interesting behavior. It is the only notebook where XGBoost hist behaves worse than XGBoost. The reason could be because the number of features is high, 2048, and that could be causing a memory overhead. LightGBM and the standard version of XGBoost can manage this high number of features, so there is no overhead. You can try to use a higher complexity to improve the performance. For example, setting `max_depth=8` in XGBoost, `max_leaves=2**8` in XGBoost hist and `num_leaves=2**6` in LightGBM. This will cause an overhead in XGBoost hist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 (Strata)",
   "language": "python",
   "name": "strata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
